{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Bidirectional,\n",
    "    Dense, Dropout, Layer, Softmax, GlobalAveragePooling1D,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "\n",
    "# ===========================================================\n",
    "# 1. Carregar dataset e preparar textos\n",
    "# ===========================================================\n",
    "df = pd.read_csv(\"/Users/matheusmota/src/github/msc/msc-proj/data/academic_works.csv\")\n",
    "df = df.head(100).copy()\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df[\"included\"] = df[\"included\"].astype(bool)\n",
    "df[\"text\"] = (df[\"title\"].fillna(\"\") + \" \" +\n",
    "              df[\"keywords\"].fillna(\"\") + \" \" +\n",
    "              df[\"abstract\"].fillna(\"\"))\n",
    "\n",
    "y = np.array(df[\"included\"].to_list())\n",
    "\n",
    "# ===========================================================\n",
    "# 2. Tokenização\n",
    "# ===========================================================\n",
    "max_words = 20000\n",
    "max_len = 300\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df[\"text\"].to_list())\n",
    "sequences = tokenizer.texts_to_sequences(df[\"text\"].to_list())\n",
    "x_seq = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# ===========================================================\n",
    "# 3. Carregar embeddings (GloVe)\n",
    "# ===========================================================\n",
    "embeddings_index = {}\n",
    "with open('../data/word_vectors/glove/glove.6B.300d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = 300\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Divisão dos dados: 70/20/10\n",
    "# ===========================================================\n",
    "# 70% treino + 30% (val + teste)\n",
    "x_train_full, x_temp, y_train_full, y_temp = train_test_split(\n",
    "    x_seq, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "# Dentro dos 30%, separar 2/3 para validação (20% total) e 1/3 para teste (10% total)\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=(1/3), stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 5. Modelo com atenção Bahdanau\n",
    "# ===========================================================\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "        self.softmax = Softmax(axis=1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        score = tf.nn.tanh(self.W1(query) + self.W2(values))\n",
    "        attn_weights = self.softmax(self.V(score))\n",
    "        context = attn_weights * values\n",
    "        context = tf.reduce_sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "def build_model(bidirectional=True, use_attention=True):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=False\n",
    "    )(inputs)\n",
    "\n",
    "    if bidirectional:\n",
    "        x = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(embedding)\n",
    "    else:\n",
    "        x = LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedding)\n",
    "\n",
    "    if use_attention:\n",
    "        x = BahdanauAttention(100)(x, x)\n",
    "    else:\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dropout(0.5 if not bidirectional else 0.02)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# ===========================================================\n",
    "# 6. Experimentos\n",
    "# ===========================================================\n",
    "experiments = [\n",
    "    (\"SVM with SGD + TF-IDF\", \"SVM\"),\n",
    "    (\"Bi-LSTM + GloVe\", (True, False)),\n",
    "    (\"Bi-LSTM + GloVe + Attention\", (True, True)),\n",
    "    (\"LSTM + GloVe\", (False, False)),\n",
    "    (\"LSTM + GloVe + Attention\", (False, True)),\n",
    "]\n",
    "\n",
    "# ===========================================================\n",
    "# 7. Função de simulação com K-Fold\n",
    "# ===========================================================\n",
    "from typing import Tuple, TypedDict\n",
    "\n",
    "class ResultOut(TypedDict):\n",
    "    Dataset: str\n",
    "    Classifier: str\n",
    "    TP: int\n",
    "    FP: int\n",
    "    TN: int\n",
    "    FN: int\n",
    "    N: int\n",
    "    P: float\n",
    "    R: float\n",
    "    F2: float\n",
    "    WSS95: float\n",
    "\n",
    "def simulate_model(name: str, config: str | Tuple[bool, bool]) -> ResultOut:\n",
    "    if config == \"SVM\":\n",
    "        vect = TfidfVectorizer(ngram_range=(1, 3))\n",
    "        X_tfidf = vect.fit_transform(df[\"abstract\"].to_list())\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        clf = SGDClassifier(loss='hinge', penalty='l2', max_iter=1000, random_state=42)\n",
    "        y_scores = cross_val_predict(clf, X_tfidf, y, cv=cv, method='decision_function')\n",
    "    else:\n",
    "        bidirectional, attention = config\n",
    "        model = build_model(bidirectional, attention)\n",
    "        lr = 1e-4 if bidirectional else 3e-4\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # --- K-Fold apenas sobre o conjunto de treino ---\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        y_scores = np.zeros_like(y_train_full, dtype=float)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(x_train_full, y_train_full)):\n",
    "            print(f\"Fold {fold+1}/5\")\n",
    "            x_tr, x_val_k = x_train_full[train_idx], x_train_full[val_idx]\n",
    "            y_tr, y_val_k = y_train_full[train_idx], y_train_full[val_idx]\n",
    "\n",
    "            model_fold = build_model(bidirectional, attention)\n",
    "            model_fold.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                               loss='binary_crossentropy',\n",
    "                               metrics=['accuracy'])\n",
    "            model_fold.fit(\n",
    "                x_tr, y_tr,\n",
    "                validation_data=(x_val_k, y_val_k),\n",
    "                batch_size=64,\n",
    "                epochs=5,\n",
    "                verbose=0,\n",
    "                class_weight={0:1, 1:44}\n",
    "            )\n",
    "            y_scores[val_idx] = model_fold.predict(x_val_k).flatten()\n",
    "\n",
    "    if config == \"SVM\":\n",
    "        y_true_scores = y\n",
    "    else:\n",
    "        # Para redes, só sobre o conjunto de treino\n",
    "        y_true_scores = y_train_full\n",
    "\n",
    "    prec, rec, thresh = precision_recall_curve(y_true_scores, y_scores)\n",
    "\n",
    "    best_thresh = next((t for p, r, t in zip(prec, rec, thresh) if r >= 0.95), 0.5)\n",
    "\n",
    "    y_pred = (y_scores >= best_thresh).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_scores, y_pred).ravel()\n",
    "\n",
    "    N = tn + fp\n",
    "    P = tp / (tp + fp) if (tp + fp) else 0\n",
    "    R = tp / (tp + fn) if (tp + fn) else 0\n",
    "    F2 = (5 * P * R) / (4 * P + R) if (P + R) else 0\n",
    "    WSS95 = (N - fp)/N - (1-0.95) if N else 0\n",
    "\n",
    "    return ResultOut(\n",
    "        Dataset=\"academic_works.csv\",\n",
    "        Classifier=name,\n",
    "        TP=tp, FP=fp, TN=tn, FN=fn,\n",
    "        N=N, P=P, R=R, F2=F2, WSS95=WSS95\n",
    "    )\n",
    "\n",
    "# ===========================================================\n",
    "# 8. Execução e salvamento dos resultados\n",
    "# ===========================================================\n",
    "import time, os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "results = []\n",
    "\n",
    "for name, config in experiments:\n",
    "    print(f\"\\nRodando {name}...\")\n",
    "    start = time.perf_counter()\n",
    "    result = simulate_model(name, config)\n",
    "    end = time.perf_counter()\n",
    "    result[\"time\"] = end - start\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "folder = \"../data/results\"\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_excel(f\"{folder}/results_table.xlsx\", index=False)\n",
    "print(results_df.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16837008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25744e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7982564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae8672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c45ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdaef95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6cdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed03560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9048f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rodando BERT-base (Bi-BERT) (bert-base-uncased)...\n",
      "Fold 1/5 — BERT-base (Bi-BERT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:39<00:00,  3.91s/it]\n",
      "Epoch 2 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:38<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5 — BERT-base (Bi-BERT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:37<00:00,  3.79s/it]\n",
      "Epoch 2 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:37<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5 — BERT-base (Bi-BERT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:39<00:00,  4.00s/it]\n",
      "Epoch 2 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:44<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 — BERT-base (Bi-BERT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:40<00:00,  4.07s/it]\n",
      "Epoch 2 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:39<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5 — BERT-base (Bi-BERT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:41<00:00,  4.11s/it]\n",
      "Epoch 2 Train BERT-base (Bi-BERT): 100%|██████████| 10/10 [00:39<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rodando DistilBERT (distilbert-base-uncased)...\n",
      "Fold 1/5 — DistilBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train DistilBERT: 100%|██████████| 10/10 [00:19<00:00,  2.00s/it]\n",
      "Epoch 2 Train DistilBERT: 100%|██████████| 10/10 [00:19<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5 — DistilBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train DistilBERT: 100%|██████████| 10/10 [00:21<00:00,  2.19s/it]\n",
      "Epoch 2 Train DistilBERT: 100%|██████████| 10/10 [00:19<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5 — DistilBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train DistilBERT: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it]\n",
      "Epoch 2 Train DistilBERT: 100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 — DistilBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train DistilBERT: 100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n",
      "Epoch 2 Train DistilBERT: 100%|██████████| 10/10 [00:20<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5 — DistilBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train DistilBERT: 100%|██████████| 10/10 [00:19<00:00,  1.99s/it]\n",
      "Epoch 2 Train DistilBERT: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rodando RoBERTa-base (roberta-base)...\n",
      "Fold 1/5 — RoBERTa-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train RoBERTa-base: 100%|██████████| 10/10 [00:41<00:00,  4.11s/it]\n",
      "Epoch 2 Train RoBERTa-base: 100%|██████████| 10/10 [00:38<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5 — RoBERTa-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train RoBERTa-base: 100%|██████████| 10/10 [00:40<00:00,  4.01s/it]\n",
      "Epoch 2 Train RoBERTa-base: 100%|██████████| 10/10 [00:37<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5 — RoBERTa-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train RoBERTa-base: 100%|██████████| 10/10 [00:41<00:00,  4.10s/it]\n",
      "Epoch 2 Train RoBERTa-base: 100%|██████████| 10/10 [00:41<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 — RoBERTa-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train RoBERTa-base: 100%|██████████| 10/10 [00:40<00:00,  4.04s/it]\n",
      "Epoch 2 Train RoBERTa-base: 100%|██████████| 10/10 [00:38<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5 — RoBERTa-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train RoBERTa-base: 100%|██████████| 10/10 [00:38<00:00,  3.88s/it]\n",
      "Epoch 2 Train RoBERTa-base: 100%|██████████| 10/10 [00:38<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rodando ALBERT-base-v2 (albert-base-v2)...\n",
      "Fold 1/5 — ALBERT-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:24<00:00,  2.42s/it]\n",
      "Epoch 2 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:27<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5 — ALBERT-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n",
      "Epoch 2 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:29<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5 — ALBERT-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:28<00:00,  2.82s/it]\n",
      "Epoch 2 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:29<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 — ALBERT-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:28<00:00,  2.87s/it]\n",
      "Epoch 2 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:29<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5 — ALBERT-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n",
      "Epoch 2 Train ALBERT-base-v2: 100%|██████████| 10/10 [00:27<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Dataset            | Classifier          |   TP |   FP |   TN |   FN |   N |   P |   R |       F2 |   WSS95 |    time |\n",
      "|---:|:-------------------|:--------------------|-----:|-----:|-----:|-----:|----:|----:|----:|---------:|--------:|--------:|\n",
      "|  0 | academic_works.csv | BERT-base (Bi-BERT) |   20 |   80 |    0 |    0 |  80 | 0.2 |   1 | 0.555556 |   -0.05 | 410.187 |\n",
      "|  1 | academic_works.csv | DistilBERT          |   20 |   80 |    0 |    0 |  80 | 0.2 |   1 | 0.555556 |   -0.05 | 220.087 |\n",
      "|  2 | academic_works.csv | RoBERTa-base        |   20 |   80 |    0 |    0 |  80 | 0.2 |   1 | 0.555556 |   -0.05 | 420.127 |\n",
      "|  3 | academic_works.csv | ALBERT-base-v2      |   20 |   80 |    0 |    0 |  80 | 0.2 |   1 | 0.555556 |   -0.05 | 298.866 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time, os\n",
    "from pathlib import Path\n",
    "\n",
    "# ===========================================================\n",
    "# 1. Carregar dataset e preparar textos\n",
    "# ===========================================================\n",
    "df = pd.read_csv(\"/Users/matheusmota/src/github/msc/msc-proj/data/academic_works.csv\")\n",
    "df = df.head(100).copy()\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df[\"included\"] = df[\"included\"].astype(bool)\n",
    "df[\"text\"] = (df[\"title\"].fillna(\"\") + \" \" +\n",
    "              df[\"keywords\"].fillna(\"\") + \" \" +\n",
    "              df[\"abstract\"].fillna(\"\"))\n",
    "\n",
    "y = np.array(df[\"included\"].to_list())\n",
    "\n",
    "# ===========================================================\n",
    "# 2.1. Definir modelos HuggingFace\n",
    "# ===========================================================\n",
    "MODEL_LIST = [\n",
    "    (\"BERT-base (Bi-BERT)\", \"bert-base-uncased\"),\n",
    "    (\"DistilBERT\", \"distilbert-base-uncased\"),\n",
    "    (\"RoBERTa-base\", \"roberta-base\"),\n",
    "    (\"ALBERT-base-v2\", \"albert-base-v2\"),\n",
    "]\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "# ===========================================================\n",
    "# 3. Dataset PyTorch\n",
    "# ===========================================================\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "from typing import TypedDict\n",
    "\n",
    "class ResultOut(TypedDict):\n",
    "    Dataset: str\n",
    "    Classifier: str\n",
    "    TP: int\n",
    "    FP: int\n",
    "    TN: int\n",
    "    FN: int\n",
    "    N: int\n",
    "    P: float\n",
    "    R: float\n",
    "    F2: float\n",
    "    WSS95: float\n",
    "    time: float\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Função de simulação K-Fold para QUALQUER modelo\n",
    "# ===========================================================\n",
    "def simulate_hf_model(name: str, model_name: str) -> ResultOut:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    encodings = tokenizer(\n",
    "        list(df[\"text\"]),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encodings['input_ids']\n",
    "    # Nem todos modelos usam token_type_ids (ex: RoBERTa)\n",
    "    attention_mask = encodings['attention_mask']\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_scores = np.zeros(len(y), dtype=float)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(input_ids, y)):\n",
    "        print(f\"Fold {fold+1}/5 — {name}\")\n",
    "        ids_train, ids_val = input_ids[train_idx], input_ids[val_idx]\n",
    "        mask_train, mask_val = attention_mask[train_idx], attention_mask[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        train_dataset = BertDataset(ids_train, mask_train, y_tr)\n",
    "        val_dataset   = BertDataset(ids_val, mask_val, y_val)\n",
    "        train_loader  = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "        val_loader    = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(2):\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train {name}\"):\n",
    "                optimizer.zero_grad()\n",
    "                input_ids_ = batch['input_ids'].to(device)\n",
    "                attention_mask_ = batch['attention_mask'].to(device)\n",
    "                labels_ = batch['labels'].unsqueeze(1).to(device)\n",
    "                outputs = model(input_ids_, attention_mask=attention_mask_)\n",
    "                loss = loss_fn(outputs.logits, labels_)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        preds_fold = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids_ = batch['input_ids'].to(device)\n",
    "                attention_mask_ = batch['attention_mask'].to(device)\n",
    "                outputs = model(input_ids_, attention_mask=attention_mask_)\n",
    "                scores = torch.sigmoid(outputs.logits).cpu().numpy().flatten()\n",
    "                preds_fold.extend(scores)\n",
    "        y_scores[val_idx] = np.array(preds_fold)\n",
    "\n",
    "    prec, rec, thresh = precision_recall_curve(y, y_scores)\n",
    "    best_thresh = next((t for p, r, t in zip(prec, rec, thresh) if r >= 0.95), 0.5)\n",
    "    y_pred = (y_scores >= best_thresh).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "    N = tn + fp\n",
    "    P = tp / (tp + fp) if (tp + fp) else 0\n",
    "    R = tp / (tp + fn) if (tp + fn) else 0\n",
    "    F2 = (5 * P * R) / (4 * P + R) if (P + R) else 0\n",
    "    WSS95 = (N - fp)/N - (1-0.95) if N else 0\n",
    "\n",
    "    return ResultOut(\n",
    "        Dataset=\"academic_works.csv\",\n",
    "        Classifier=name,\n",
    "        TP=tp, FP=fp, TN=tn, FN=fn,\n",
    "        N=N, P=P, R=R, F2=F2, WSS95=WSS95,\n",
    "        time=0.0\n",
    "    )\n",
    "\n",
    "# ===========================================================\n",
    "# 5. Execução e Salvamento dos Resultados\n",
    "# ===========================================================\n",
    "results = []\n",
    "for name, model_hf in MODEL_LIST:\n",
    "    print(f\"\\nRodando {name} ({model_hf})...\")\n",
    "    start = time.perf_counter()\n",
    "    result = simulate_hf_model(name, model_hf)\n",
    "    end = time.perf_counter()\n",
    "    result[\"time\"] = end - start\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "folder = \"../data/results\"\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_excel(f\"{folder}/results_hf_models.xlsx\", index=False)\n",
    "print(results_df.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-proj-_qgJAu6J",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
